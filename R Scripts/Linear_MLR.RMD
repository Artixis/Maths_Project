---
title: "Population Analysis"
output:
  word_document: default
  pdf_document: default
date: "2022-09-25"
---


Load Libraries
```{r, warning=FALSE}
x <- c("ggmap", "rgdal", "rgeos", "maptools", "dplyr", "tidyr", "tmap", "sf", "MASS", "rgeos", "gmt", "lme4", "lmerTest", "ggplot2", "sf", "tidyverse", "gganimate")
lapply(x, library, character.only = TRUE) 
```

Load in the population data set
The population data set
```{r}
dataPop = read.csv("C:\\Users/Owner/OneDrive - Western Sydney University/Spring 2022/Mathematics Project/Data Sets (Need to organise)/populationLong.csv")
```



Manipulate the population data. Create new columns, 'density' and 'yearsince2000'
Density is calculated for each region, over each of the 20 years. So Alfredton, for example
has 20 associated samples in our data set. Each year the population, potentially, changes. 
For each observation we calculate density = population/area. 

yearsince2000 is simply the year column with 2000 subtracted from it.
```{r}
dataPop$density <- 0.0    
dataPop$density <- dataPop$pop/dataPop$area
dataPop$yearsince2000 <- dataPop$year-2000
```


Transform the density variable
A square root transformation was chosen after running
a simple linear model with density as the predictor through
the box-cox function.
```{r}
dataPop$trans_density <- sqrt(dataPop$density)
```


Load in shape file. Greater Sydney needs to be specified. If not, the shape file
will not be able to match NSW SA2 regions to the population data set, which now,
only contains NSW regions.

The shape file uses polygons to geometrically decribe Australia by its SA2 regions.
It includes various fields, the one we mainly reference is SA2_NAME16. This is how
we match the regions in the shape file to the ones in our population data set.
```{r}
syd <- st_read("C:\\Users/Owner/OneDrive - Western Sydney University/Spring 2022/Mathematics Project/Data Sets (Need to organise)/SA2_2016_AUST.shp")
greaterSyd <- syd[syd$GCC_NAME16=="Greater Sydney",]
```

```{r}
head(greaterSyd)
```



st_ centroid
"Computes a point which is the geometric center of mass of a geometry."
```{r, warning=FALSE}
centr <- st_centroid(syd)

centroid_lat <- unlist(lapply(centr$geometry,function(l) l[2]))
centroid_long <- unlist(lapply(centr$geometry,function(l) l[1]))
```

Create variables within the shape file sydist and parradist. These are the 
calculated distance of each SA2 from Sydney and Parramatta respectively.

Log these values to normalize them.

Insert these calculated values into the population data frame by matching
the two files by SA2 name.


NOTE:
Look into geodist, is the function using haversine as it's distance formula?
If not, find out which one it is. 

Regardless, a brief explaination of how distance is calculated using the 
centroids and geodist function would be useful to us and the reader/audience.
```{r}
syd$sydist <- geodist(-33.8688,151.2093,centroid_lat,centroid_long)
syd$parradist <- geodist(-33.8148,151.0017,centroid_lat,centroid_long)
dataPop$sydist <- syd$sydist[match(dataPop$name,syd$SA2_NAME16)]
dataPop$parradist <- syd$parradist[match(dataPop$name,syd$SA2_NAME16)]

dataPop$logsydist <- log(syd$sydist[match(dataPop$name,syd$SA2_NAME16)])
dataPop$logparradist <- log(syd$parradist[match(dataPop$name,syd$SA2_NAME16)])

head(dataPop$sydist)
```
```{r}
# Drop SA2's without syddist
#df <- df[!is.na(df$sydist),]
```

Run the head and tail end of the data to make sure this has worked
```{r}
head(dataPop,5)
tail(dataPop,5)
```
A brief comparison of how effective the transformation on the density variable
was. 
Compare the mean and median of the original variable. 
There must be some very large density SA2's skewing these results
```{r}
median(dataPop$density)
mean(dataPop$density)
```


However after performing the square root transformation there is a much
smaller gap between the two. 

Ideally in an evenly distributed data set or variable, the mean and median
will be very close to equal.
```{r}
median(dataPop$trans_density)
mean(dataPop$trans_density)
```


--------------------------------------------------------------------------------
Visualize the data
--------------------------------------------------------------------------------


Plot the variables not transformed
```{r, warning = FALSE}
plot(dataPop$sydist, dataPop$density, xlab = "Distance from Sydney", 
     ylab = "Density",main="Scatter plot of Distance vs Density (Raw Data)" , col = "#CC99FF", addRegLine=TRUE)
abline(lm(dataPop$density ~ dataPop$sydist), col="#56B4E9", lwd = 2.5)
```



Now graph our transformed variables of density and Sydney distance 
to see if there's a visual indication of a linear relationship between the 
two. 

```{r, warning = FALSE}
plot(dataPop$logsydist, dataPop$trans_density, xlab = "Logged Distance from Sydney", 
     ylab = "Square root of Density",main="Scatter plot of Distance vs Density" , col = "#56B4E9", addRegLine=TRUE)
abline(lm(dataPop$trans_density ~ dataPop$logsydist), col="#CC99FF", lwd = 2.5)
```

From the there does seem to be a negative relationship between the two variables.
This suggests that the density of SA2 regions decreases as the move away from 
Sydney. 
However it is important to acknowledge that many of the observations lay far 
away from the purple regression line that has been overlayed onto the plot. 


--------------------------------------------------------------------------------
Models
--------------------------------------------------------------------------------


Linear model of the data
Predicting density with the distance from Sydney as the predictor. 


```{r}
pop_lm = lm(trans_density ~ logsydist, data = dataPop)
summary(pop_lm)
```

```{r}
library("report")
```

```{r}
report(pop_lm)
```


Interpreting the summary output: 



Residuals: 
There is not a very strong symmetrical distribution in the residuals. This is 
actually what we'd expect given our previous scatterplot because this implies
that the model will predict points that fall far away from the actual observed 
points. 

Coefficients:
Estimate
Analyzing the coefficients is a little bit confusing since the variables have been 
transformation. 
For now I'll describe each variable in terms of "units" rather than kms or
density(as pop/area). 
The intercept is the expected density units when considering the average distance
units in logsydist. 
The logsydist estimate coefficient suggest that for every one unit of distance from
Sydney, the density will drop by -13.2925 units (As we saw from our regression line
in the scatter plot)


Std. Error
"The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. "
Looking at logsydist, the distance can vary by 0.1351 units in our prediction.

t-values 
Our t-values are far away from zero and large compared to the standard errors, 
which can indicate a relationship. 
These values are used to calculate the p-value. 

Pr(>t)
Both p-value a highly significant to this model. We can conclude that there 
is a relationship between distance and density.


Residual standard error: 
"Residual Standard Error is measure of the quality of a linear regression fit. Theoretically, every linear model is assumed to contain an error term E."
In our model that actual distance can deviate from the true regression line 
by 15.33 units. So we can calculate the percentage error from this using
the intercept(Mean density) is 18.0635%, not bad?
So any prediction will be off by 18.0635%

R-Squared
"A side note: In multiple regression settings, the R2 will always increase as more variables are included in the model. Thatâ€™s why the adjusted R2 is the preferred measure as it adjusts for the number of variables considered." 

Our Adjusted R-squared = 0.5916 
This indicates that our model explains 59.16% of the variance found in the 
response variable(density) can be explained by the predictor variable(distance).


F-Statistic:
9686 on 1 and 6686 DF
Again this is another indicator of a relationship between predictor and 
response. 



```{r}
parraLinear = lm(trans_density ~ logparradist, data = dataPop)
summary(parraLinear)
plot(dataPop$trans_density~dataPop$logparradist)
```


```{r}
yearLin = lm(trans_density ~ yearsince2000, data = dataPop)
summary(yearLin)
```

```{r}
plot(trans_density~yearsince2000,data=dataPop)
```


Multiple Linear Regression

Try a multiple linear regression to predict density.
```{r}
pop_mlr1 = lm(trans_density ~ logsydist + yearsince2000, data = dataPop)
summary(pop_mlr1)
```

```{r}
plot(pop_mlr1)
```


Interpreting the summary output: 
The output is extremely similar to the previous model. 

Differences:
The yearsince2000 variable has slightly less significance in the model 
compared to the previous two. However it's p-value is not so low as the 
omit the variable entirely from the model. 
This model actually accounts for more of the variance in the data, albeit only
0.05% of the variance(From the Adjusted R-squared). 
The coefficients for the intercept and distance are almost the same as the 
previous model. 
The yearsince2000 only slightly effects our change in  density. That is, for 
every yearsince2000, we only see a 0.09625 change in our density unit. 


^ yearsince200 is not transformed so we can acknowledge the year change as a
unit, the other two variables are transformed, hence me decribing them as 
'units'.

```{r}
MLR_Model = lm(trans_density ~ logsydist + logparradist + yearsince2000, data = dataPop)
summary(MLR_Model)
```

```{r}
MLR_Model2 = lm(trans_density ~   logparradist + yearsince2000, data = dataPop)
summary(MLR_Model2)
```
